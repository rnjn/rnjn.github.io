<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[ranjan sakalley]]></title>
  <link href="http://rnjn.github.io/atom.xml" rel="self"/>
  <link href="http://rnjn.github.io/"/>
  <updated>2015-07-30T14:57:32+05:30</updated>
  <id>http://rnjn.github.io/</id>
  <author>
    <name><![CDATA[rnjn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    
    <title type="html"><![CDATA[On Postels law and managing change]]></title>
    <link href="http://rnjn.github.io/2015/07/on-postels-law-and-managing-change.html"/>
    
    <updated>2015-07-30T13:49:52+05:30</updated>
    <id>http://rnjn.github.io/2015/07/on-postels-law-and-managing-change</id>
    
    <content type="html"><![CDATA[<h2>On Postel&rsquo;s law and Change</h2>

<p>Postel&rsquo;s law ~1981<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>,</p>

<blockquote><p>&ldquo;Be conservative in what you do, be liberal in what you accept from others.&rdquo; (RFC 793)</p></blockquote>

<p>A webservice accepting a message with a defined schema (xml/json/other) may choose to do one of these when it encounters messages with extra nodes/properties &ndash;</p>

<ul>
<li>be conservative and discard</li>
<li>be liberal and accept</li>
</ul>


<p>Of the two, there are more proponents for the latter than the former. There are many reasons thrown at the conservatives, one of them that has practical implications is change in schema. Before we go further lets state the obvious &ndash; if there is never ever going to be a change in the schema of a message, being conservative is just the same as being liberal. Except that the &ldquo;never ever&rdquo; <strong>never</strong> really happens, there&rsquo;s always some change in schema. The choice of being liberal or conservative mandates thinking about how you deal with change in schema, and that&rsquo;s a good thing.</p>

<p>Lets talk about change. Producers and consumers of messages agree on a schema and then continue with their business. When a producer decides to change the schema to improve the featureset, it is generally considered a good practice to communicate the change using software versioning, a version being a quantum of change as agreed. If the responsibility on propagating the change lies with the producer: the producer distributes and manages upgrades of the service client, they need not worry about being liberal. This sometimes is true for custom enterprise software produced and consumed internally. However, this is small subset of software produced these days. Most of the services written today, provide an API and a reference library, and allow consumers to write their own clients.</p>

<p>The implication of being conservative about accepting messages then is that all known and unknown clients of a service have to upgrade in tandem or face downtime, when there is just one version of the service up and running. (Strictness doesn&rsquo;t just mean discard messages with additional properties, it also mandates not accepting deficient messages as per the schema). A better way to propagate change may be to host multiple versions of a service, and that just means <strong>we elevated Postel&rsquo;s law from a producer&rsquo;s view of a particular instance of the service, to a consumer oriented view of liberally providing a service</strong>, albeit via conservative instances of different versions of the same service. The question then is, how many versions of a service would you maintain at the same time? With changes big and small, the number of versions quickly proliferate and cost of hosting and maintenance is not cheap.</p>

<p>The choice of tolerant reader<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> design doesn&rsquo;t really much help with big changes, since big changes in schema that are needed to service new features etc. need client updates anyway, and can be designed as above. Also, accepting a string that represents the full message, if you were to take Postel&rsquo;s law to an extreme would require a lot of plumbing and would cost higher to work with and make secure. This design choice shines when it comes to adding smaller changes. Lets take a real example. A service that we have been working on receives messages that contain a Doctor&rsquo;s name along with other patient diagnoses related data. A new feature request from end users that we have to implement immediately is to add phone numbers and other contact information of said doctor so that the doctor can be contacted at a later date, drives a change in schema. If we follow the multiple strict service versions approach, we would probably add a new version for this. The cost of managing different versions very quickly starts affecting the quantum of change released to consumers, and producers start designing big releases instead of multiple smaller changes. A liberal design might help contain hosted service instance proliferation by letting older clients send messages they are used to sending, while upgraded clients can send new information too. At a later date, if we for some reason need to downgrade back to a previous version, we do not need to downgrade all clients. This flexibility helps enormously when managing servers and delivering small changes continuously.</p>

<p>Another thing, being liberal isn&rsquo;t easy. A conservative service assumes that schema validation is enough to avoid runtime errors (this assumption is not entirely correct<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>). A conservative service can assume that it never has to service clients that expect a different version of it. Versions can be deprecated by just not installing them, or removing a server. A service designed to be liberal however can only provide schema guidelines and has to wait to extract all possible information from a message to be able move forward in most cases. Also, a liberal design that enables servicing multiple versions of clients with just one instance of the service needs a lot thought. The producer has to manage change very closely, and since deprecation is a code change, these decisions take more time to production. Its also very hard to keep an eye on how many versions are being supported at the same time. <strong>A combination of a small set of available versions each being liberal to an extent provides a good alternative</strong>. Also, a good way to work on some of the difficulties with liberal reader design are consumer driven contracts.</p>

<p>There is another counterpoint<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> to Postel&rsquo;s law and liberal reader design &ndash; security experts advice us to be conservative in what we accept, contrary to Postel&rsquo;s law. Indiscriminately accepting any message ofcourse has consequences. Yet within the bounds of a sane secure message protocol, a message data schema that allows subelements to be optionally present can be worked with. (Indiscriminately following any advice mostly hurts, including this one).</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="https://en.wikipedia.org/wiki/Postels_law">Postel&rsquo;s law</a> &ndash; wikipedia<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Martin Fowler describes decoupled collaboration <a href="http://martinfowler.com/bliki/TolerantReader.html">Tolerant reader</a><a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p><a href="http://blog.iancartwright.com/2006/11/schema-validation-offers-false-sense-of.html">Schema validation offers a false sense of security</a> &ndash; Ian Cartwright<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>Eric Allman&rsquo;s counterpoints to the principle <a href="http://cacm.acm.org/magazines/2011/8/114933-the-robustness-principle-reconsidered/fulltext">The robustness principle reconsidered</a><a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Pretty Python list comprehensions]]></title>
    <link href="http://rnjn.github.io/2014/05/pretty-python-list-comprehensions.html"/>
    
    <updated>2014-05-12T11:40:23+05:30</updated>
    <id>http://rnjn.github.io/2014/05/pretty-python-list-comprehensions</id>
    
    <content type="html"><![CDATA[<p>Python list comprehensions are by far the simplest and most readable loop expressions that I have worked with. Here&rsquo;s an example where I have a list of lists of lists (corpus &ndash;> documents &ndash;> sentences) where I need to remove some items (called stop_words here) from the sentences.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">corpus</span> <span class="o">=</span> <span class="p">[[[</span><span class="n">word</span>
</span><span class='line'>            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_stop_words</span><span class="p">]</span>
</span><span class='line'>         <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">document</span><span class="p">]</span>
</span><span class='line'>       <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">]</span>
</span><span class='line'>                                                      
</span></code></pre></td></tr></table></div></figure>


<p>Notice the array brackets added after each for expression so the program retains the same structure.</p>

<p>Another example, if I have to flatten such a deep list-</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="k">return</span> <span class="p">[</span><span class="n">word</span>
</span><span class='line'>          <span class="k">for</span> <span class="n">document</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_corpus</span>
</span><span class='line'>         <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">document</span>
</span><span class='line'>       <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Building a positional model]]></title>
    <link href="http://rnjn.github.io/2014/04/building-a-positional-model.html"/>
    
    <updated>2014-04-03T12:38:28+05:30</updated>
    <id>http://rnjn.github.io/2014/04/building-a-positional-model</id>
    
    <content type="html"><![CDATA[<p><em>Part of a series on extracting information from semi-structured text, starts <a href="../03/analysing-semi-structured-text-and-extracting-features.html">here</a></em></p>

<h2>Introduction</h2>

<p>Blocking and labelling using known words stored in a knowledge base lets us extract known information from a document. However, for words that are new to the knowledge base, like a new car brand or model in the example domain, blocking and labelling fails. To be able to categorise or label new words, we take a probabilistic approach. Given a sizeable number of documents, we build a positional model, that helps us find an appropriate label for a new word given its position in the document.</p>

<h2>Preparing data</h2>

<p>Before we start calculating probabilities, we aggregate a sizeable corpus of documents, lets say 50000 or more, and run our blocking and labelling step. We then search for documents which have &ldquo;unknown&rdquo; blocks and remove them from our set. We do this assuming these are &lt; 10% of the corpus. If this is not true, the quality of the knowledge base needs to be improved, and we keep iterating the cycle below</p>

<blockquote><p>improve KB &ndash;> Block and Label > % of documents not labelled completely > improve KB&hellip;</p></blockquote>

<p>till we reach this goal.</p>

<p>Once we pass this step, we move on to building a representative matrix of documents with each cell denoting the position of a label. For a labelled document like below &ndash;</p>

<blockquote><p>[&ldquo;sell&rdquo;, &ldquo;hyundai&rdquo;, &ldquo;santro&rdquo;, &ldquo;2007&rdquo;, &ldquo;green&rdquo;, &ldquo;tyres&rdquo;,  &ldquo;leather&rdquo;, &ldquo;seats&rdquo;, &ldquo;price&rdquo;, &ldquo;300000&rdquo;, &ldquo;contact&rdquo;, &ldquo;91xxxxxxx&rdquo;]</p></blockquote>

<p>the row vector (of labels by position) in the matrix that denotes this document looks like</p>

<blockquote><p>[&ldquo;ad_type&rdquo;, &ldquo;brand&rdquo;, &ldquo;model&rdquo;, &ldquo;year&rdquo;, &ldquo;colour&rdquo;, &ldquo;features&rdquo;, &ldquo;price&rdquo;, &ldquo;contact&rdquo;]</p></blockquote>

<p>Now, to reduce the size of the matrix in memory, we may represent each label by a unique number. and so our representative row vector looks like</p>

<blockquote><p>[2,3,4,5,6,7,8,9]</p></blockquote>

<p>(we reserve 1 for &ldquo;unknown&rdquo;)</p>

<p>The length of each document may differ, so we find the longest document, and we pad all documents with 0&rsquo;s to fit the matrix width. An example matrix of 10 documents across 10 labels may look like &ndash;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="p">[[</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">6</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">6</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">7</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">6</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">7</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">9</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">6</span><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>where each row represents a document and each column represents a position in the document, and numbers denote a label.</p>

<h2>Positional Model</h2>

<p>To build a vector for positional probabilities for a label we follow</p>

<blockquote></blockquote>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">for</span> <span class="n">each</span> <span class="n">label</span><span class="p">:</span>
</span><span class='line'>  <span class="k">for</span> <span class="n">each</span> <span class="n">position</span><span class="p">:</span>
</span><span class='line'>      <span class="n">probability</span><span class="p">[</span><span class="n">position</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">occurence</span> <span class="n">of</span> <span class="n">this</span> <span class="n">label</span><span class="p">)</span><span class="o">/</span><span class="n">number</span> <span class="n">of</span> <span class="n">documents</span>
</span></code></pre></td></tr></table></div></figure>


<p>Over 50000 documents and 12 labels, with a maximum size of 150 tokens, traditional looping does not perform well. We turn to the <a href="http://www.numpy.org">numpy</a> python library to help us crunch these numbers faster for us.</p>

<p>First, we derive a matrix for each label, which represents the position of this label only across documents. We call this matrix the label sieve. For the matrix above, choosing label 5, the sieve should look like &ndash;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
</span><span class='line'><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<p>where 1 denotes the presence of label 5 in a document, and 0 denotes absence.</p>

<p>The following code does this for us</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="n">label_sieves</span> <span class="o">=</span> <span class="p">[(</span><span class="n">matrix</span> <span class="o">==</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<blockquote><p>numpy matrices can be queried like &ndash;> matrix == n to result in a boolean matrix for the presence of n. the result * 1 just converts this to the above.</p></blockquote>

<p>Now its easy for us to sum all column vectors in the matrix above, where each column represents a position in the document. Again, numpy provides faster calculations, as below</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="n">label_position_totals</span> <span class="o">=</span> <span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label_sieve</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class='line'>                             <span class="k">for</span> <span class="n">label_sieve</span> <span class="ow">in</span> <span class="n">label_sieves</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>and then we just divide by the count of documents.</p>

<p>Our final positional matrix may look like &ndash;</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'> <span class="p">[[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.3</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.4</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.3</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">],</span>
</span><span class='line'> <span class="p">[</span> <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">,</span>  <span class="mf">0.1</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.2</span><span class="p">,</span>  <span class="mf">0.</span> <span class="p">]]</span>
</span></code></pre></td></tr></table></div></figure>


<h2>Extras</h2>

<p>We may derive a different model if we calculate the probability as &ndash;</p>

<blockquote></blockquote>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="k">for</span> <span class="n">each</span> <span class="n">label</span><span class="p">:</span>
</span><span class='line'>  <span class="k">for</span> <span class="n">each</span> <span class="n">position</span><span class="p">:</span>
</span><span class='line'>      <span class="n">probability</span><span class="p">[</span><span class="n">position</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">occurence</span> <span class="n">of</span> <span class="n">this</span> <span class="n">label</span><span class="p">)</span><span class="o">/**</span><span class="nb">sum</span><span class="p">(</span><span class="n">occurance</span> <span class="n">of</span> <span class="nb">any</span> <span class="n">label</span> <span class="n">other</span> <span class="n">than</span> <span class="mi">0</span><span class="p">)</span><span class="o">**</span>
</span></code></pre></td></tr></table></div></figure>


<p>To make this change, we first build a sieve which denotes the presence of all labels, and then sum column-wise to get a denominator vector by position.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="n">sieve</span> <span class="o">=</span> <span class="p">(</span><span class="n">matrix</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span>
</span><span class='line'>    <span class="n">denominators</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sieve</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Our final probability calculation looks like</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">probabilities</span> <span class="o">=</span> <span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">label_totals</span><span class="p">,</span> <span class="n">denominators</span><span class="p">)</span>
</span><span class='line'>    <span class="k">for</span> <span class="n">label_totals</span> <span class="ow">in</span> <span class="n">label_position_totals</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>where numpy.divide does an item-wise division.</p>

<p>This calculation produces skewed numbers for a column where the presence labels in the column is very low. To normalise the probabilities in this case, the previous method is better.</p>

<h2>Performance</h2>

<p>Traditional looping method to build this model for around 50000 documents, 14 labels and a maximum size of 144 words (50000 X 144 size matrix), takes around 2 seconds with our modest python skills. Using numpy, as coded above, we could do this in milli-seconds. The added advantage being expressive code.</p>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Blocking and labelling]]></title>
    <link href="http://rnjn.github.io/2014/04/blocking-and-labelling.html"/>
    
    <updated>2014-04-01T16:25:56+05:30</updated>
    <id>http://rnjn.github.io/2014/04/blocking-and-labelling</id>
    
    <content type="html"><![CDATA[<p><em>Part of a series on extracting information from semi-structured text, starts <a href="../03/analysing-semi-structured-text-and-extracting-features.html">here</a></em></p>

<h2>Introduction</h2>

<p>With a knowledge base of known terms built, we now move to identifying labels in a test corpus that can be assigned to a sub-sequence of strings. Given an accurate KB, this step can extract all information, given all sub-sequences in a document are known. We follow a simple sequential algorithm to identify blocks and label them into categories. We mark sub-sequences that we cannot categorise as &ldquo;unknown&rdquo;. We leave identification of &ldquo;unknown&rdquo; sub-sequences to later steps.</p>

<h2>Alogrithm</h2>

<p>Assuming that a document (an advertisement in the example domain) has already been tokenised, we follow the steps below.</p>

<blockquote><ol>
<li>pick the first token, find it in the KB</li>
<li>if found, create a tuple of label:[list of words] and insert word in the list</li>
<li>get the next token, find the label. if it belongs to the same label, append the word to the list.</li>
<li>if the token doesn&rsquo;t belong to the same label, go back to 2.</li>
<li>if no label is found, mark it as &lsquo;unknown&rsquo;. do not append &lsquo;unknown&rsquo; tuples together.</li>
<li>for each new token, either append to the previous token&rsquo;s tuple, or create a new tuple.</li>
</ol>
</blockquote>

<p>The result is a sequencial list of tuples (blocks) which together represent the document.</p>

<h2>Improvements</h2>

<p>The simple algorithm above works well when there is no ambiguity in finding labels for a word. There are 2 cases which add complexity &ndash;
* A token is not in the KB. We leave the probabilistic models to help us assign a label to such word/sequence
* A token is can be categorised under 2 labels.</p>

<p>To help fix a label to the latter, we assign it an intermediate label, look for the next or previous token&rsquo;s label and see if this token can be continuance. For instance, in our example domain, a 5-6 digit numeric string can represent a price or kilometers driven for a vehicle. We assign it a token called &lsquo;number&rsquo;. We then analyse the previous and next tokens to see if this &lsquo;number&rsquo; can be merged with the previous or next tokens&#8217; labels. If this &lsquo;number&rsquo; comes after &lsquo;price&rsquo;, we assume that the token under analysis can be categorised as &lsquo;price&rsquo;. If this &lsquo;number&rsquo; has no neighbours that can logically follow, we revert to marking it as &lsquo;unknown&rsquo;.</p>

<p>The following code explains this process.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class='line'>    <span class="n">token</span> <span class="o">=</span> <span class="n">tokens</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class='line'>    <span class="n">block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_block</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
</span><span class='line'>    <span class="n">token_index</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">while</span><span class="p">(</span><span class="n">token_index</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)):</span>
</span><span class='line'>        <span class="n">next_block</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_block</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">token_index</span><span class="p">])</span>
</span><span class='line'>        <span class="n">merged_block</span> <span class="o">=</span> <span class="n">block</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">next_block</span><span class="p">)</span>
</span><span class='line'>        <span class="k">if</span> <span class="ow">not</span> <span class="n">merged_block</span><span class="p">:</span>
</span><span class='line'>            <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</span><span class='line'>            <span class="n">block</span> <span class="o">=</span> <span class="n">next_block</span>
</span><span class='line'>        <span class="k">else</span><span class="p">:</span>
</span><span class='line'>            <span class="n">block</span> <span class="o">=</span> <span class="n">merged_block</span>
</span><span class='line'>
</span><span class='line'>        <span class="n">token_index</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class='line'>
</span><span class='line'>    <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>The merge method in the block object takes care of combining intermediate states to known labels or discarding them as &ldquo;unknown&rdquo;</p>

<p>For an advertisement tokenised as</p>

<blockquote><p>[&ldquo;sell&rdquo;, &ldquo;hyundai&rdquo;, &ldquo;santro&rdquo;, &ldquo;2007&rdquo;, &ldquo;green&rdquo;, &ldquo;tyres&rdquo;,  &ldquo;leather&rdquo;, &ldquo;seats&rdquo;, &ldquo;price&rdquo;, &ldquo;300000&rdquo;, &ldquo;contact&rdquo;, &ldquo;91xxxxxxx&rdquo;]</p></blockquote>

<p>blocking and labelling output may be</p>

<p><img class="center" src="../../images/blocking.png"></p>

<h2>Results</h2>

<p>We have seen that blocking and labelling has been able to extract information from a document to the accuracy of close to 85%. This is true for documents which have known text strings. The rest can be marked down to new strings which we do not know about. For numerical sequences we have seen our accuracy numbers change depending on the way numbers are mentioned in the document.</p>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Building a knowledge base for text segmentation]]></title>
    <link href="http://rnjn.github.io/2014/04/building-a-knowledge-base-for-text-segmentation.html"/>
    
    <updated>2014-04-01T01:02:30+05:30</updated>
    <id>http://rnjn.github.io/2014/04/building-a-knowledge-base-for-text-segmentation</id>
    
    <content type="html"><![CDATA[<p><em>Part of a series on extracting information from semi-structured text, starts <a href="../03/analysing-semi-structured-text-and-extracting-features.html">here</a></em></p>

<h2>Introduction</h2>

<p>A tedious yet crucial step in the text segmentation approach is building a knowledge base(KB) of known terms and their labels. Labelling (grouping and naming) words is also domain specific, for instance labelling words/phrases for analysis of car sales classified advertisements may be quite different from real estate classified advertisements. Its a cost that is incurred repeatedly as and when new domains are acquired. I present an improvement on the completely manual process below, but first, lets talk about the basic work. As before, examples are from a car buying/selling classified ad corpus.</p>

<h2>KB for text labels</h2>

<p>A knowledge base is a dictionary of tokens grouped by labels they fall under. We use the term &lsquo;label&rsquo; to define the type of a sub-sequence of tokens, which together join to form a sentence. For instance in the following advertisement</p>

<blockquote><p>I want to sell Hundai Santro 2007 model green coloured car in mint condisan. New tyres, regular servicing, good AC and leatherseats. Price Rs 3Lakhs negotiable Contact +91xxxxxxxxxx</p></blockquote>

<p>the sub-sequence &ldquo;Hyundai&rdquo; is actually the &ldquo;brand&rdquo; of a vehicle. A KB for domain might therefore look like &ndash;</p>

<blockquote></blockquote>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">knowledge_base</span> <span class="o">=</span> <span class="p">{</span> <span class="s">&quot;brand&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;hyundai&quot;</span><span class="p">,</span> <span class="s">&quot;maruti&quot;</span><span class="p">,</span> <span class="s">&quot;suzuki&quot;</span><span class="p">,</span> <span class="s">&quot;ford&quot;</span><span class="o">......</span><span class="p">]</span> <span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>(obviously, there are better data structures to store a KB, this is just conceptual)</p>

<p>Similarly, add words to the KB under &ldquo;model&rdquo;, &ldquo;fuel type&rdquo; and other text labels. Words can belong to more than one label.</p>

<h2>KB of numbers</h2>

<p>While words can be compared and classified using a simple dictionary as above, numbers are a completely different story. For instance, to build a KB as above for the label &ldquo;Price&rdquo;, an infinite number of numbers need to be listed. A Regex might be a better respresentation for the class of numbers. Moreover, a corpus might have a limited number of classes of numeric sequences in tokens and hence we may build a dictionary of Regexes which addresses a class each. In the example domain, the 4 different classes we encounter are</p>

<blockquote><ul>
<li>price &ndash; 5-7 digits</li>
<li>year &ndash; 4 digits</li>
<li>phone numbers &ndash; 10 digits</li>
<li>miles/kms driven &ndash; 5-7 digits</li>
</ul>
</blockquote>

<p>Writing Regex matchers for these classes is easy &ndash;</p>

<blockquote></blockquote>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>    <span class="n">number_matcher</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;^\d{5,9}$&#39;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">phone_number</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;^\d{10}$&#39;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">year</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="s">r&#39;^(19|20)\d{2}$&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<p>We encounter complexity in mapping numbers to labels when 2 classes of numeric strings overlap. In the example above, &ldquo;price&rdquo; and &ldquo;kms driven&rdquo; are overlapping classes. To overcome this, we add these steps &ndash;</p>

<blockquote><ul>
<li>include words like [&lsquo;kms&rsquo;, &lsquo;miles&rsquo;, &lsquo;kilometers&rsquo;&hellip;.] and [&lsquo;price&rsquo;, &lsquo;cost&rsquo;, &lsquo;Rs&rsquo;&hellip;.] etc. in the knowledgebases for the two labels.</li>
<li>create an intermediate label, lets say &ldquo;number&rdquo;</li>
<li>categorise a &ldquo;number&rdquo; depending on surrounding words (from the first step)</li>
</ul>
</blockquote>

<h2>Improvements</h2>

<p>Although the process of building a KB seems simple, it really is not. At the least its time consuming and is a repeated cost everytime a new domain is acquired. Reading and labelling a large corpus of text is also error prone. To this end, an improvement can be to compute vector respresentation of tokens using word2vec<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> &ndash; an unsupervised learning tool.</p>

<p>word2vec (python interface <a href="http://radimrehurek.com/gensim/models/word2vec.html">gensim</a>) builds a vocabulary from a corpus and then learns vector representations of words. Vector respresentation is very useful in calculating distance between words, for instance to allow the following (given a lot of relevant text)&ndash;</p>

<blockquote><p>vec(&lsquo;Madrid&rsquo;) &ndash; vec(&lsquo;Spain&rsquo;) + vec(&lsquo;France&rsquo;) is closer to vec(&lsquo;Paris&rsquo;) than to any other word vector.</p></blockquote>

<p>We take their vector representations and use a Eucledian clustering technique like
<a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means clustering</a> to identify groups of similar words. Scipy provides a variety of clustering algorithms that might be useful, we choose <a href="http://docs.scipy.org/doc/scipy/reference/cluster.vq.html">k-means</a> for simplicity and then change depending on feedback.</p>

<blockquote><p>word2vec has immense potential and is a tool (and approach) to watch for text analysis solutions.</p></blockquote>

<p>The number of dimensions and clusters created are customisable parameters. The resulting output may contain a bunch of clusters which are very similar to the text-only knowledge bases as mentioned above. As an example, with a corpus of around 57000 car buying/selling classified advertisements, with word2vec calculating vector representations in a 100 dimensional space and k-means clustering configured to find out 50 clusters, we were able to find 12 clear clusters for brand names, models, colours, car feature, location of seller etc. These clusters provide an excellent start to builing a knowledge base and cut short the time taken to &lsquo;learn&rsquo; a domain. A larger corpus might help to identify words with high cohesiveness with higher accuracy.</p>

<p>Since a number of clusters identified might not be of value while building our KBs, it would be difficult to automate this approach to remove all manual intervention. Its also important to note that the parameters mentioned above need to tuned and clustering tested using statistical tools to find the best possible combination. Automating this process is not trivial, and is an area of interest and improvement for future.</p>

<h2>Summary</h2>

<p>Building a KB for a domain involves indentifying and classifying different classes of strings in a corpus of text from said domain. This is a very tedious exercise which requires manual analysis of large volumes of text. Different data structures might be needed to store the KBs, a simple dictionary would do for text, but it needs to be modified to include numbers efficiently, Regex matchers are invaluable. Finally, word2vec provides a definite boost in productivity when &lsquo;learning&rsquo; and new domain and building a KB for it.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="https://code.google.com/p/word2vec/">word2vec</a> : Tool for computing continuous distributes representations of words. A tool to watch.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Tokenisation++]]></title>
    <link href="http://rnjn.github.io/2014/03/tokenisation-plus-plus.html"/>
    
    <updated>2014-03-31T18:16:59+05:30</updated>
    <id>http://rnjn.github.io/2014/03/tokenisation-plus-plus</id>
    
    <content type="html"><![CDATA[<p><em>Part of a series on extracting information from semi-structured text, starts <a href="../03/analysing-semi-structured-text-and-extracting-features.html">here</a></em></p>

<h2>Introduction</h2>

<blockquote><p>corpus &ndash; a collection of written texts, especially the entire works of a particular author or a body of writing on a particular subject.</p></blockquote>

<p>Given a corpus of text, the first step is to <a href="https://en.wikipedia.org/wiki/Tokenize">tokenise</a> it. To elaborate, I would take an example of a corpus of classified advertisements for selling and buying cars. The added work on top of regular tokenisation addresses wayward spellings and sentence formations common in the Indian web-based classified ads corpora. Subsequent work in analysing the text would work on the tokens we obtain. I assume that you have a list of strings, each an advertisement. I used python to code this, you will see references to python libraries that have been helpful.</p>

<p>A typical classified advertisement selling a car follows</p>

<blockquote><p>I want to sell Hundai Santro 2007 model green coloured car in mint condisan. New tyres, regular servicing, good AC and leatherseats. Price Rs 3Lakhs negotiable Contact +91xxxxxxxxxx</p></blockquote>

<p>The ordering of steps below help me explain the approach. The code shouldn&rsquo;t follow logical order, it wouldn&rsquo;t be performant.</p>

<h2>Regular tokenisation</h2>

<p>We start with breaking the advertisement down into a list of sentences. Then we break each sentence down into a list of words. A typical approach for English would be break sentences down by finding one of {&lsquo;;&rsquo;, &lsquo;:&rsquo;, &lsquo;,&rsquo;, &lsquo;.&rsquo;, &lsquo;!&rsquo;, &lsquo;?&rsquo;} and then break them down further by using spaces. However, different languages have different ways of ending a sentence or separation between words. There&rsquo;s further complexity if we include local flavour or contexts for e.g. a period between two numbers is not a full-stop. Instead of mulling through different variations, we can delegate this work to the nltk <a href="http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktSentenceTokenizer">PunktSentenceTokeniser</a>, which is trained on a collection of English text. Our task it then reduced to the following</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'>            <span class="k">for</span> <span class="n">ad</span> <span class="ow">in</span> <span class="n">ads</span><span class="p">:</span>
</span><span class='line'>                <span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
</span><span class='line'>              <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
</span><span class='line'>                  <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure>


<h2>The ++</h2>

<p>Now, it makes sense to look at the corpus in general and create some general rules about tokens, which boost accuracy of subsequent analysis. This step might also be called a data cleaning step. Here are a few that come in handy for the example domain &ndash;</p>

<p>for text input</p>

<blockquote><ul>
<li>all tokens should be in lower case</li>
<li>all tokens would represent individual words, not combinations</li>
<li>all tokens would contain human readable unicode</li>
<li>no periods in between a word, unless its numeric</li>
</ul>
</blockquote>

<p>for numerical input</p>

<blockquote><ul>
<li>all numerical input will be in numeric characters (e.g. convert 3Lakhs to 300000)</li>
<li>no commas in numerical input</li>
<li>no special characters in a phone number string</li>
</ul>
</blockquote>

<p>We logically craft these in {before/during/after} the sentence and word tokenisation. Many of the above, like the numerical token rules can be performed easily using Regex matches and substitutions. These rules may not be premeditated. Indeed, we depend on the subsequent analysis process to for feedback on the tokenisation process to update the rules. For instance, if we encounter a situation where spelling variations and mistakes lower accruacy of analysis, we might add the following &ndash;</p>

<blockquote><ul>
<li>use only one way to spell a word</li>
<li>try best to correct spellings</li>
</ul>
</blockquote>

<p>Now, there are different ways to spell correct. A very simple approach is to use a language dictionary in conjunction with following process for domain specific words &ndash;</p>

<blockquote><ul>
<li><p>create a <a href="https://en.wikipedia.org/wiki/Histogram">histogram</a> of all words. The Counter class in python <a href="https://docs.python.org/2/library/collections.html">collections</a> is a fast and easy way to do this.</p>

<p>  <code>
      word_histogram = Counter(tokens)
 </code></p></li>
<li>if a word exists in the dictionary, don&rsquo;t do anything.</li>
<li>for each word not in the dictionary, iterate over the histogram and find words which are closest in terms of <a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein distance</a>. Use a threshold number for choosing the list of candidates, to improve accuracy.</li>
<li>find the most frequently used word in the above list and replace the less frequently used one.</li>
<li>insert this into a secondary dictionary for future use.</li>
</ul>
</blockquote>

<p>The Enchant library (<a href="http://pythonhosted.org/pyenchant/">pyenchant</a> interface for python) provides a great interface to an English dictionary. Notably, it allows for new words to be added and it suggests words given an incorrect one.</p>

<p>This approach depends on the assumption that there are more correctly spelled words than not. Unfortunately, sometimes this is not correct and analysis algorithms may have to address this problem, or as a final solution manual correction may be required. There are other customisations that might help, like choosing a different distance measure to fit the domain of words and errors. In a non-English locale a phonetic similarity measure might help, for instance to figure that &lsquo;condisan&rsquo; above is &lsquo;condition&rsquo;.</p>

<p>The example advertisement copied above showcases two more issues &ndash; misspelling a proper noun (Hyundai v/s Hundai) and joined word (leather seats v/s leatherseats). For the former, when using Enchant, its advisable to add a list of proper nouns to the enchant dictionary, so as to improve suggestions. We also add another step of checking distance from this list of proper nouns (which might contain brand names etc.). The latter however, involves a performance intensive solution of creating smaller sets of words from a bigger word (if its not found in our customised dictionary) and then checking if in a particular combination of these sets, a high number of smaller words are found in our customised dictionary. In the case above, we might want to limit our sets to <a href="https://en.wikipedia.org/wiki/Ngram">bigrams</a> first-</p>

<blockquote><p>[{lea, therseats}, {leat, herseats}, {leath, erseats}, {leathe, rseats}, {leather, seats}&hellip;..]</p></blockquote>

<p>and clearly the set {leather, seats} is a strong contender. In case we don&rsquo;t find a meaningful set from bigrams, we create trigrams and so on.</p>

<h2>Stop words and other measures</h2>

<p>As a penultimate step, we remove from our list of tokens, words which have negative or no significant effect on the final outcome of text analysis. We keep a list of such <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a> and when we encounter them, we remove them from our token list. An example of such a list can be &ndash;</p>

<blockquote><p>a,able,about,across,after,all,almost,also,am,among,an,and,
any,are,as,at,be,because,been,but,by,can,cannot,could,dear,
did,do,does,either,else,ever,every,for,from,get,got,had,has,
have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,
just,least,let,like,likely,may,me,might,most,must,my,neither,
no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,
say,says,she,should,since,so,some,than,that,the,their,them,then,
there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,
when,where,which,while,who,whom,why,will,with,would,yet,you,your</p></blockquote>

<p>Each problem domain has its own set of stop words, once again results of our next steps in analysis will help building a better list. For e.g., viz. the automobile related classified advertisements, the following may be considered stop words</p>

<blockquote><p>model,showroom,fast,comfort,trend&hellip;</p></blockquote>

<p>Finally, we choose a threshold token size/word length, below which a token doesn&rsquo;t qualify to be part of the analysis steps. For the example domain, upto 2 letter words are not significant, and either hurt performance or accuracy. We also remove all remaining letters that are not numeric/alphabetic.</p>

<h2>Output</h2>

<p>Example output for the example advertisement may be &ndash;</p>

<blockquote><p>[&ldquo;sell&rdquo;, &ldquo;hyundai&rdquo;, &ldquo;santro&rdquo;, &ldquo;2007&rdquo;, &ldquo;green&rdquo;, &ldquo;mint&rdquo;, &ldquo;condition&rdquo;, &ldquo;new&rdquo;, &ldquo;tyres&rdquo;, regular&#8221;, &ldquo;servicing&rdquo;, &ldquo;good&rdquo;, &ldquo;leather&rdquo;, &ldquo;seats&rdquo;, &ldquo;price&rdquo;, &ldquo;300000&rdquo;, &ldquo;negotiable&rdquo;, &ldquo;contact&rdquo;, &ldquo;91xxxxxxx&rdquo;]</p></blockquote>

<h2>Summary</h2>

<p>As part of the tokenisation and cleaning process, we break text into a list sentences each of which is a list of words (called tokens). We remove stop/insignificant words, and we code in a bunch of rules that help our analysis process, for instance &ndash; dealing with numbers etc. We craft this tokenisation process based on experience, then fit it into a solution and gather feedback, and then use this feedback to update the process. Rinse, repeat.</p>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Analysing semi-structured text and extracting features]]></title>
    <link href="http://rnjn.github.io/2014/03/analysing-semi-structured-text-and-extracting-features.html"/>
    
    <updated>2014-03-31T18:02:28+05:30</updated>
    <id>http://rnjn.github.io/2014/03/analysing-semi-structured-text-and-extracting-features</id>
    
    <content type="html"><![CDATA[<p>&lt;&lt; work in progress. TODO: sequential model, inference and future work >></p>

<h2>Introduction</h2>

<p>There is significant interest in extracting information from unstructured or semi-structured text like classified advertisements, so it can be stored in a query-able form for further analysis. In the specific example case of classified advertisements, used through this set of pages, this can have a variety of applications &ndash; from improving user experience, better placements, to creating price prediction tools using machine learning techniques. A classified advertisment can be called a semi-structured document as it generally has the following parts &ndash;</p>

<blockquote><ul>
<li>someone is [Contact]</li>
<li>selling/buying [Type of Ad]</li>
<li>something [Brand, Model]</li>
<li>used/new [Year of manufacture]</li>
<li>features [Color, Condition etc.]</li>
<li>domain specific features like AC/non-AC, stereo system, seat types etc. [Features]</li>
</ul>
</blockquote>

<p>More often than not, these parts are not placed neatly in order, and advertisements may differ significantly so as to make it harder to model them using a set of Regex parsers. In a test corpus for around 57,000 car selling/buying classified ads, rarely a case where a set structure been followed existed. But there is a (or a limited number of) abstract semi-structure(s) that could be seen. We exploit these to extract information by text segmentation. We follow an unsupervised probabilistic approach called ONDUX<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> that helps us build a probabilistic model for classified ads for a domain given enough data. This approach is well suited for semi-structured smaller documents like classified advertisements &ndash; we identify known segments in a document using a knowledge base and we use a probabilistic model to identify unknown segments. This probabilistic model is built using a sizeable test corpus.</p>

<p>The following diagram explains the end-to-end process &ndash;
<img class="center" src="../../images/process.png"></p>

<p>The details are introduced under the following heads, and each has a more explanatory document.</p>

<blockquote><ul>
<li>Tokenisation</li>
<li>Building a knowledge base</li>
<li>Blocking and labelling</li>
<li>Building a Positional model</li>
<li>Building a Sequencial model</li>
<li>Inference</li>
</ul>
</blockquote>

<h2>Tokenisation</h2>

<p>Tokenisation is the process of converting a corpus to a list of advertisements, with each advertisement being a list of <em>significant words</em>. Word significance can be defined in terms of how our analysis can be affected by a word in terms of accuracy of output as well as the performance of our program. For example, we remove language and domain stop words, as well as words which are below a certain length because these words have either negative or zero effect on accuracy and performance. We also perform some data clean up like spell checking, which are detailed <a href="./tokenisation-plus-plus.html">here</a>.</p>

<h2>Building a knowledge base</h2>

<p>The approach we follow to label segments in a sentence is heavily dependent on an existing knowledge base or a dictionary of {label : [words]}. For the example domain, the following dictionary is a simple KB for the &ldquo;brand&rdquo; label:</p>

<blockquote></blockquote>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='python'><span class='line'><span class="n">knowledge_base</span> <span class="o">=</span> <span class="p">{</span> <span class="s">&quot;brand&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s">&quot;hyundai&quot;</span><span class="p">,</span> <span class="s">&quot;maruti&quot;</span><span class="p">,</span> <span class="s">&quot;suzuki&quot;</span><span class="p">,</span> <span class="s">&quot;ford&quot;</span><span class="o">......</span><span class="p">]</span> <span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>This step takes a significant amount of time, in an otherwise unsupervised model building approach. More on this detailed <a href="../04/building-a-knowledge-base-for-text-segmentation.html">here</a>.</p>

<h2>Blocking and labelling</h2>

<p>Once the knowledge base is built, we start our unsupervised modelling process. The first step is to identify labels in a test corpus that can be assigned to a sub-sequence of strings. Given an accurate KB which contains a comprehensive list of words, this step helps extracting all known information from a new document. The algorithm and some customisations are listed <a href="../04/blocking-and-labelling.html">here</a>.</p>

<h2>Building a positional model</h2>

<p>While blocking and labelling helps us extract known information from a document (an advertisement in this case), it is unable to categorise words that are new. For instance, when our program encounters an advertisement which addresses a new car brand, it may not be able to categorise and extract the right &ldquo;brand&rdquo; information. To improve accuracy in such cases, we build a probabilistic model that addresses the position of labels in a typical advertisement. First, we build what we call the positional model for our labels. The positional model is a simple data structure that stores the probability of each label occuring at a certain position in an advertisement, and we learn this by blocking and labelling a sizeable corpus of advertisements. More <a href="../04/building-a-positional-model.html">here</a>.</p>

<h2>Buiding the sequential model</h2>

<p>The positional model gives us a general idea of where labels are placed in a document. But this model is not enough to identify/label unknown segments, as the observations are general and individual documents may not follow observed patterns. To strengthen our approach, we add another probabilistic model which addresses the sequencing of labels, namely the probability of a label occuring before or after another label, irrespective of the position of the labels. Combined together, these three probabilities would help us corner the right label for an unknown sub-sequence.</p>

<h2>Inference</h2>

<p>Once our probabilistic models are built, we are ready to extract information from a document. For our example domain, this program takes in an advertisement and gives back a simple object which stores all the information that we can extract from it. We call this process our inference step. We follow the step below to infer.</p>

<ul>
<li>all known information can be extracted by blocking and labelling the document</li>
<li>we mark things we cannot label as &ldquo;unknown&rdquo;</li>
<li>since the positional and sequential probabilities of an &ldquo;unknown&rdquo; sub-sequence or string are mutually exclusive, we can calculate its probable category using a logical disjunctive formula to find a label to be in that position surrounded by known labels. Specifics here.</li>
</ul>


<h2>Results</h2>

<p>We see significant accuracy of extracted information out of classified documents from our blocking and labelling step. As of today, an average of 80% labels in an advertisement are correctly identified. We see further improvements when our probabilistic model kicks in to identify labels that our first step could not identify.</p>

<h2>Future work</h2>

<ul>
<li>explore opportunities to automate KB creation using tools like word2vec</li>
<li>implement improvements to the current probabilistic models.<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></li>
<li>test our approach acrross different domains in a similar setup and verify our assumptions.</li>
<li>explore other algorithms for this work</li>
<li>scaling up to build a model over a sizeable corpus</li>
</ul>

<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p><a href="http://link.springer.com/chapter/10.1007/978-3-319-02597-1_4#page-1">ONDUX</a>: On demand unsupervised learning for information extraction- Eli Cortez et al. (2010)<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p><a href="http://crpit.com/confpapers/CRPITV137Huynh.pdf">Proximity based positional model</a> &ndash; Hyunh and Zhou (ADC 2013)<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Simpler emacs setup]]></title>
    <link href="http://rnjn.github.io/2014/02/simpler-emacs-setup.html"/>
    
    <updated>2014-02-25T11:12:40Z</updated>
    <id>http://rnjn.github.io/2014/02/simpler-emacs-setup</id>
    
    <content type="html"><![CDATA[<div dir="ltr" style="text-align: left;" trbidi="on">
  <div dir="ltr" style="text-align: left;" trbidi="on">I moved from my older, cumbersome emacs setup to a package.el based one. I currently use an Emacs 24.3&nbsp;
    <a href="http://emacsformacosx.com/">build
    </a> for Mac OS X (which comes with package.el). Its cleaner -
    <br />
    <br />
    <ol style="text-align: left;">
      <li>I just list the packages I need in my init file - no submodules business
      </li>
      <li>Run through the list to find if a package is installed or not. If not, download and install package from elpa/melpa/marmalade
      </li>
      <li>For now, an added manual step when removing a package is to delete related folder from the /elpa/ directory
      </li>
    </ol>
    <div>Here&#8217;s a gist which explains (I maintain this setup at <a href="https://github.com/rnjn/dotemacs/">github</a>)
    </div>
    <div><script src='https://gist.github.com/9205481.js'></script>
<noscript><pre><code>(setq installed-packages &#39;(
			   color-theme
			   color-theme-solarized
			   yasnippet
			   auto-complete
			   js2-mode
			   ac-js2
			   ))

(setq package-archives &#39;((&quot;gnu&quot; . &quot;http://elpa.gnu.org/packages/&quot;)
			 (&quot;marmalade&quot; . &quot;http://marmalade-repo.org/packages/&quot;)
			 (&quot;melpa&quot; . &quot;http://melpa.milkbox.net/packages/&quot;)))

;; init elpa
(package-initialize)

;; refresh contents if not present
(when (not package-archive-contents) (package-refresh-contents))

;;; install packages from the list if not installed already
(dolist (package installed-packages)
  (when (and (not (package-installed-p package))
	     (assoc package package-archive-contents))
    (message &quot;package is %s&quot; package)
    (package-install package)))

</code></pre></noscript></div>
    
    <div>
      <br />
    </div>
  </div>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Introducing automation to an existing large team]]></title>
    <link href="http://rnjn.github.io/2011/10/introducing-automation-to-existing.html"/>
    
    <updated>2011-10-24T00:00:00+05:30</updated>
    <id>http://rnjn.github.io/2011/10/introducing-automation-to-existing</id>
    
    <content type="html"><![CDATA[<div class='post'>
    <div dir="ltr" style="text-align: left;">
        <span style="font-size: x-small;"><i><span style="font-size: small;">A reader on my <a href="http://www.infoq.com/articles/test-automation-continuous-delivery">article</a> at infoq asked an interesting question - introducing automation to a big project which has been worked on for some time. <span id="quote_77221">I plan to write more posts on this topic, have a lot of thoughts, but </span>here&#8217;s my immediate answer -</span> </i>
        </span>
        <br />
        <br />
        <span id="quote_77221">A very strong actionable technique that I have seen work well is that you create a small/minimal <a href="http://en.wikipedia.org/wiki/Smoke_testing#Software_development">smoke test suite</a> for you larger app. You can decide on what comprises a suite of smoke test -</span>
        <br />
        <ul style="text-align: left;">
            <li>
                <span id="quote_77221">Basic user actions - some 4-5 use cases without which a user cannot use your application. (for e.g. on an eCommerce site, my smoke tests would comprise of login, user registration, search, end-to-end purchase and payment).</span>
            </li>
        </ul>
        <ul style="text-align: left;">
            <li>
                <span id="quote_77221">Integration points - choose must-have integrations - like payment gateway (same eCommerce motif), oauth for authentication against Facebook or some such. Write smoke tests that touch these integrations to know if every build that you process is verified for these.</span>
            </li>
        </ul>
        <ul style="text-align: left;">
            <li>
                <span id="quote_77221">Attack a small part/feature of the application. You can choose the most important, the most buggy etc. Write tests only for this part.</span>
            </li>
        </ul>
        <ul style="text-align: left;">
            <li>
                <span id="quote_77221">&nbsp;Concentrate on the currently &#8220;under-development&#8221; work, which will give the team immediate value in terms of coverage. This is a bit counter-intuitive as there would be a lot of changes while you work. But I have seen this work wonders at times, and your team starts worrying about test maintenance from the first step on.</span>
            </li>
        </ul>
        <span id="quote_77221">You can choose more ways according to your project, better still, mix and match. But make sure you choose the most important 5-10 and not 50-100 test cases for your smoke suite. Your smoke suite shouldn&#8217;t take much time to run aim for &lt; 5 minutes. The correct way to work with these would be to make these smoke tests as part of your CI process so that the team get immediate benefits out of the smoke tests.
            <br />&nbsp;</span>
        <br />
        <span id="quote_77221">Your team will start seeing value in automating regression scenarios</span>
        <span id="quote_77221">with this smoke suite in</span>
        <span id="quote_77221">place. From then on, you can expand your suite from smoke upwards, adding more complex or more detailed scenarios. Another thing to note on big complex applications - aim for small tests, get quick wins. Then combine to make bigger ones. This will keep your team involved and constantly working towards covering every possible nook and keep being excited about it. But do make sure everything is part of CI, because if its not, then people lose faith.
            <br />
            <br />Finally, as ever, keep showing value, make sure the stakeholders understand the importance, you can do this by sending them reports, and talk about gains etc. If your team buys in with the idea, there&#8217;s no stopping.</span>
    </div>
</div>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Why is test automation the backbone of Continuous Delivery?]]></title>
    <link href="http://rnjn.github.io/2011/08/why-is-test-automation-the-backbone-of.html"/>
    
    <updated>2011-08-25T00:00:00+05:30</updated>
    <id>http://rnjn.github.io/2011/08/why-is-test-automation-the-backbone-of</id>
    
    <content type="html"><![CDATA[<div class='post'>
<div dir="ltr" style="text-align: left;" trbidi="on">Software testing and verification needs a careful and diligent process of impersonating an end user, trying various usages and input scenarios, comparing and asserting expected behaviours. Directly, the words &#8220;careful and diligent&#8221; invoke the idea of letting a computer program do the job. Automating certain programmable aspects of your test suite thus can help software delivery massively. In most of the projects that I have worked on, there were aspects of testing which could be automated, and then there were some that couldn&#8217;t. Nonetheless, my teams could rely heavily on our automation suite when we had one, and spend our energies testing aspects of the application we could not cover with automated functional tests. Also, automating tests helped us immensely to meet customer demands for quick changes, and subsequently reaching a stage where every build, even ones with very small changes went out tested and verified from our stable. As Jez rightly says in his excellent text about <a href="http://continuousdelivery.com/">Continuous Delivery</a>, automated tests &#8220;take delivery teams beyond basic continuous integration&#8221; and on to the path of continuous delivery. In fact, I believe they are of such paramount importance, that to prepare yourself for continuous delivery, you must invest in automation. In this text, I explain why I believe so.<br /><br /><b>How much does it cost to take one small change to production?</b><br /><br />As the complexity of software grows, the amount of effort verifying changes as well as features already built grows, at least linearly. This means that testing time is directly proportional to the number of test cases needed to verify correctness. Thus, adding new features means that testing either increases the time it takes a team to deliver software from the time development is complete, or it adds cost of delivery if the team adds more testers to cover the increased work (assuming all testing tasks are independent of each other). A lot of teams, and I have worked with some, tackle this by keeping a pool of testers working on &#8220;regression&#8221; suites throughout the length of a release verifying if new changes break already built functionality. This is not only costly, its ineffective, slow and error prone. <br /><br />Automating test scenarios where you can lets you cut this time/money it takes to verify if a user&#8217;s interaction with the application works as designed. At this point, let us assume that a reasonable number of your test scenarios can be automated, say 50%, as this is often the least bound in software projects.If your team can and does automate this set to a certain number of repeatable tests, it frees up people to concentrate more on immediate changes. Also, lets suppose that it takes as much as 3 hours to run your tests (it should take as less as possible, less than 20 minutes even). This directly impacts the amount of time it takes to push a build out to customers. Increasing the number of automated tests, and also investing in getting the test-run time down, your agility and ability to respond increases massively, while also reducing the cost. I explain this with some very simple numbers (taking an average case) below -<br /><br /><u>Team A</u> -<br />1. Number of scenarios to test - 500 and growing.<br />2. Number of minutes to setup environment for a build - 10 minutes.<br />3. Number of minutes to test one scenario - 10 minutes.<br />4. Number of testers in your team &nbsp;- 5. <br />5. Assume that there are no blockers.<br /><br />If you were to have no automated tests, the amount of time it would take to test one single check-in (in minutes) -&nbsp;&nbsp;&nbsp;&nbsp;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 10 + (500*10)/5 = 1010 minutes. <br /><br />This is close to 2 working days (standard 8 hours each). Not only is this costly, it means that if developers get feedback 2 days later. This kind of a setup further encourages mini-waterfalls in your iteration.<br /><br /><u>Team B</u> -<br />Same as Team A, but we&#8217;ve automated 50% (250 test cases) of our suite. Also, assume that running these 250 test cases take a whopping 3 hours to complete.<br />Now, the amount of time it would take to test one single check-in (in minutes) -<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; task 1 (manual) - &nbsp;&nbsp;10 + (250*10)/5 = 510 minutes.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; task 2 (automated) - 10 + 180 minutes.<br /><br />This is close to 1 working day. This is not ideal, but just to prove the fact about reduced cost, we turned around the build one day before. We halved the cost of testing. We also covered 50% of our cases in 3 hours, and that &nbsp;Now to a more ideal and achievable case -<br /><br /><u>Team C</u> - <br />Same as Team B, but we threw in some good hardware to run the tests faster (say 20 minutes), and automated a good 80% of our tests (10% cannot be automated and 10% is new functionality).<br />Now, the amount of time it would take to test one single check-in (in minutes) -<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task 1 (manual) - 10 + (100*10)/5 = 210 minutes.<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;task 2 (automated) - 10 + 20 minutes = 30 minutes.<br /><br />So in effect, we cover 80% of our tests in 30 minutes, and overall take 3.5 hours to turn around a build. Moreover, the probability of finding a blocker earlier, by covering 80% of our cases in 30 minutes, means that we can suspend further manual testing if we need to. Our costs are lower, we get feedback faster. This changes the game a bit, doesn&#8217;t it.<br /><br /><b>Impossibility of verification on time</b><br /><br />Team A that I mentioned above would need 50 testers to certify a build in less than 2 hours. That cost is not surprisingly unattractive to customers. In most cases, it is almost impossible to turn around a build from development to delivery within a day, without automation. I say almost impossible as this would prove to be extremely costly in cases where it is. So, if assuming that my team doesn&#8217;t automate and hasn&#8217;t got an infinite amount of money, every time a developer on the team checks-in one line of code, our time to verify a build completely increases by hours and days. This discourages a manager to schedule running these tests every time on every build, which consequently decreases the quality coverage for builds, and the amount of time bugs stay in the system. It also, in some cases I have experienced, dis-incentivizes frequent checking in of code, which is not <a href="http://www.martinfowler.com/bliki/FrequencyReducesDifficulty.html">healthy</a>.<br /><b><br />Early and often feedback</b><br /><br />One of the most important aspects of automation is the quick feedback that a team gets from a build process. Every check-in is tested without prejudice, and the team gets a report card as soon as it can. Getting quicker feedback means that less code gets built on top of buggy code, which in turn increases the credibility of software. To extend the example of teams A, B and C above - <br /><br />For Team A - the probability of finding a blocker on day one is 1/2. Which basically means that there is a good risk of finding a bug on the second day of testing, which completely lays the first days of work to waste. That blocker would need to be fixed, and all the tests need to be re-verified. The worst case is that a bug is found after 2 days of an inclement line of code getting checked-in.<br /><br />For Team B - the worst case is that you find a blocker during the last few hours of the day. This is still much better than for Team A. Better still, as 50% of test cases are automated, the chance of finding a blocker within 3 hours is very high (50%). This quick feedback lets you find and fix issues faster, and therefore respond to customer requests very quickly.<br /><br />For Team C - the best case of all 3. The worst scenario is that Team C will know after 3 hours if they checked-in a blocker. As 80% of test cases are automated, by 20 minutes, they would know that they made a mistake. They have come a long way from where Team A is, 20 minutes is way better than 2 days. <br /><br /><br /><b>Opportunity cost</b><br /><br />Economists use an apt term - <a href="http://en.wikipedia.org/wiki/Opportunity_cost">opportunity cost</a> to define what is lost if one choice amongst many is taken. The opportunity cost of re-verifying tedious test cases build after build is the loss of time spent on exploratory testing. More often than not a bug leads to many, but by concentrating on manual scenarios, and while catching up to do so, testers hardly find any time to create new scenarios and follow up on issues. Not only this, it is imperative that by concentrating on regression tests all the time, testers spend proportionately less time on newer features, where there is a higher probability of bugs to be found. By automating as much as possible, a team can free up testers to be more creative and explore an application from the &#8220;human angle&#8221; and thus increase the depth of coverage and quality. On projects I have worked on, whenever we have had automated tests aiding manual testing I have noticed better and in-depth testing which has results in better quality.<br /><br />Another disadvantage is that manual testing involved tedious re-verification of the same cases day after day. Even if they are creative to distribute tests to different people every day, the cycle would inadvertently repeat after a short period of time. Testers have less time to be creative, and therefore their jobs less gratifying. Testers are creative beings and their forte is to act as end-users and find new ways to test and break an application, not in repeating a set process time after time. The opportunity cost in terms of keeping and satisfying the best testers around is enormous without automation.<br /><br /><b>Error prone human behaviour</b><br /><br />Believe it or not, even the best of us are prone to making mistakes doing our day to day jobs. Given how good or bad we are it, the probability of making a mistake while working is higher or lower, but mostly a number greater than 0. It is important to keep this risk in mind while ascertaining the quality of a build. Indeed, human errors are generally behind most bugs that we see in software applications that we see, error during development and testing. Computers are extremely efficient doing repetitive tasks - they are diligent and careful, which makes automation a risk mitigation strategy.<br /><br /><b>Tests as executable documentation</b><br /><br />Test scenarios provide an excellent source of knowledge about the state of an application. Manual test results provide a good view of what an application can do for an end user, and also tell the development team about quirky components in their code. There are two components to documenting test results - showing what an application can do, and upon failures, documenting what fails and how, so its easy to manage application abnormalities. If testers are diligent and make sure they keep their documentation up to date (another overhead for them), it is possible to know the state of play through a glance at test results. The amount of work increases drastically with failures, as testers then need to document each step, take screenshots, maybe even videos of crash situations. Adding the time spent on these increase the cost of making changes, in fact in a way the added cost dis-incentivizes documenting the state with every release. <br /><br />With automated tests, and by choosing the right tools, the process of documenting the state of an application becomes a very low cost affair. Automation testing tools provide a very good way of executing tests, collating results in categories, publishing results to a web page, and also let you visualize test result data to monitor progress and get relevant feedback from test result data. With tools like Twist, Concordian, Cucumber and the lot, it becomes really easy to show your test results, even authoring, to your customers and this reduces the losses in translation with the added benefit of customer getting more involved in application development. Upon failures, a multitude of testing tools automate the process of taking screenshots, even videos, to document failures and errors in a more meaningful way. Results could be mailed to people, much better served as RSS feeds per build to people who are interested. <br /><br /><b>Technology facing tests</b><br /><br />Testing non-functional aspects of an application - like testing application performance upon a user action, testing latency over a network and its effect on an end-users interaction with the application etc. have traditionally been partially automated (although very early during my work life I have sat with a stop watch in my hand to test performance, low-fi but effective). It is easy to take advantage of automated tests and reuse them to tests such non-functional tests. For example, running an automated functional test over a number of times can tell you average performance of an action on your web-page. The model is easy to set up, put a number of your automated functional tests inside a chosen framework that lets you setup and probe non-functional properties while the tests are run. Testing and monitoring aspects like role-based security, effects of latency, query performance etc. can all be automated by re-using an existing set of automated tests, an added benefit. <br /><br /><b>Conclusion</b><br /><br />On your journey to Continuous Delivery, small and big, you would have to take many steps. My understanding and suggestion would be to start small with a good investment on a robust automation suite, give it your best people, cultivate habits in your team that respect tests and results, build this backbone first, and then off you would be. Have a smooth ride. <br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /></div></div>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Who should write functional tests?]]></title>
    <link href="http://rnjn.github.io/2011/01/who-should-write-functional-tests.html"/>
    
    <updated>2011-01-03T00:00:00+05:30</updated>
    <id>http://rnjn.github.io/2011/01/who-should-write-functional-tests</id>
    
    <content type="html"><![CDATA[<div class='post'>
Functional testing code is more often than not treated as a second class citizen. Delivery teams tend to ignore problems with test code over a period of time, and worry more about test results. This leads to poor code quality and bad test architecture, which in turn hurts the maintainability of a test suite. Its this negative feedback cycle that a team should be worried about. In my opinion, treating test code as responsibly as we treat functional code fixes this.<br /><br /><b>Understand the importance</b> - The greatest benefit of a robust automation suite is the ability to deliver quickly, even continuously, with a known state of software quality. Given that business today demands quicker delivery of software to production, it seems quite imperative that teams take the one big block of code that enables them to do so very seriously.<br /><br /><b>Complexity</b> - Lets face it, writing an effective and maintainable automation suite is not an easy job. Just like writing production code, it needs thoughtful design and care. Test infrastructure design is an ongoing process, and just like functional architecture it needs continuous re-design based on functional and non-functional changes. An ill-designed suite is a waste of time and money (no surprises there), it also influences decision makers incorrectly. You should be asking (some of) your best people to work on it.<br /><br /><b>Expertise</b> - There are two parts to a maintainable functional suite, the framework (test infrastructure) which defines a DSL, and then the tests written using that DSL. The drawing below shows what I mean by this division. <br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/_E_bGfyPw5O4/TR3K2Uk0hWI/AAAAAAAAB8o/SHJEvnWk3D0/s1600/functionaltests.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="332" src="http://2.bp.blogspot.com/_E_bGfyPw5O4/TR3K2Uk0hWI/AAAAAAAAB8o/SHJEvnWk3D0/s640/functionaltests.png" width="640" /></a></div><br /><br />&nbsp;&nbsp;Dividing coding responsibilities based on the lines above is quite necessary. Developers are your experts when it comes to writing frameworks and maintaining them. Testers are really good at writing intentions and maintaining functional sanity. In fact with this virtual ownership division, testers become customers for developers, which distributes responsibility quite elegantly.<br /><br /><b>Ownership</b> - If developers don&#8217;t write functional tests, its imperative that they are not super-motivated to fix them when they fail as new features are added/amended. Having different owners of different parts of code not only increases cycle time, it also reduces the quality of feedback that the developers get from automation. <br /><br /><b>Work definition and process</b> - When we write functional code, we tend to have well-written narratives, which have acceptance criteria, and are treated well. When writing the framework for the automation suite, or a new functional test, such narratives should be well defined and the same workflow should be maintained. Any work done on functional tests should not become a secondary process for the team. Such treatment&nbsp;de-prioritizes work on functional tests at the outset itself, and should therefore be avoided.<br /><br /><b>Conclusion</b> - My conclusion here is that people who are skilled to write production code should design and write test infrastructure/frameworks, while analysts should write test specifications, as they are best placed to do these. And also, work on functional tests (stories etc.) should be treated as mainstream functional work.</div>

]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Why teams lose faith in their functional test suite]]></title>
    <link href="http://rnjn.github.io/2010/12/why-teams-lose-faith-in-their.html"/>
    
    <updated>2010-12-06T00:00:00+05:30</updated>
    <id>http://rnjn.github.io/2010/12/why-teams-lose-faith-in-their</id>
    
    <content type="html"><![CDATA[<div class='post'>
<div>In my opinion, there are three main reasons why a functional automation suite loses its value (or the respect that a delivery team should pay it). This leads to a variety of problems, but I will save that for a later post. </div><div><br /></div><div>The reasons for me are -</div><div><br /></div><div>1. <b>Non-deterministic tests </b>- <i>run the same test again (without changing the code) and the test gives different results</i>. </div><div><br /></div><div>   This, in my opinion is the least attractive aspect, a true demotivator for a delivery team to believe in its functional test suite. One of the reasons I have seen non-deterministic failures is when the tests run on hardware that&#8217;s performance is non-deterministic, for example VMs which share hardware resources, and hence are dependent on how the other VMs on the same host are performing during the test run. Sometimes, tests are non-deterministic when they access external systems which are non-deterministic. Unless testing the robustness of the system under test, stubbing out such external dependencies have worked out well. Another reason is just bad tests, like tests which depend on the time which they are running at (unless functionally driven to), tests which have time-outs which are not well researched and more. Sometimes the tools used to drive tests are non-deterministic (we found a couple of them driving Silverlight tests) which makes it really difficult to believe in the results of the tests you write. </div><div>   All non-deterministic tests can and should be fixed, but I have seen the effort to fix these being directly proportional to the amount of customer involvement/value to delivery. People also cite them as &#8220;random failures&#8221; and ignore them. I believe that every developer who calls a program &#8220;random&#8221; loses respect by a notch everytime they call it so.</div><div><br /></div><div>2. <b>Lot of failures in a functional suite</b> - <i>if there are 50 out of 100 tests failing, the team&#8217;s belief dwindles.</i></div><div><br /></div><div>   The first question that you ask is how did we end up here? This has a lot to do with the <a href="http://en.wikipedia.org/wiki/Broken_windows_theory">broken window theory</a>. If all tests pass and the next check-in fails some tests, the team immediately works to fix them. If a regular bunch keep failing and are thus ignored, any new failures will become part of the ignored tests. Again, this can be fixed. Reporting and analysing the failed tests, and constant effort improves the state and builds a positive cycle. However, as before, the intent to improve the state depends on how valuable the tests are for delivery/customer.</div><div><br /></div><div>3. <b>Time taken to run the suite once</b> - <i>feedback comes in very late</i>.</div><div><br /></div><div>   If a single run tests a lot of check-ins, it becomes really hard to analyse/debug the failures. If this is coupled with 1 and 2 above, it results in a total loss of faith, as fixing takes a lot of time and frustration levels go up. So even if this might not be a primary reason, coupled with the ones above, it deteriorates the dependency the delivery team has on automated functional tests to certify a good build. One way to fix performance is to distribute tests to a grid infrastructure, where different nodes are responsible for a subset of tests, thus parallelizing the execution and cutting down run time.</div><div><br /></div><div>The factors listed above can all be fixed, with investigation and investment. My experience has been that the intent to spend time and money on fixing these issues is directly proportional to how much a customer believes in them, and asks the team to be accountable for. Teams can also increase their own accountability by exposing results and analysis. If, for example, a delivery team publishes test results as part of release notes, or publishes/exposes the results in some other way to customers, this creates awareness as well as increases accountability of its work on the suite. That for me is the optimal solution.</div><div><br /></div><div>Thanks - to Martin Fowler, Manish Kumar and Chethan V for helping me crystallize my thoughts on this.  </div><div><br /></div><div><br /></div></div>
]]></content>
    
  </entry>
  
  <entry>
    
    <title type="html"><![CDATA[Is your Functional suite done right?]]></title>
    <link href="http://rnjn.github.io/2009/02/is-your-functional-suite-done-right.html"/>
    
    <updated>2009-02-10T00:00:00+05:30</updated>
    <id>http://rnjn.github.io/2009/02/is-your-functional-suite-done-right</id>
    
    <content type="html"><![CDATA[<div class='post'>
On the last two projects that I have worked on, both being fairly sized in terms of people (40+), I have seen enormous effort being spent on functional testing. The effort, though not completely wasted, hasnt yielded proportional gains in terms of quality improvements and quicker feedback on a higher integration level. The following list tries to address issues and my take on fixing them.<br /><br /><span class="Apple-style-span" style="font-weight: bold;">Separation of Concerns</span><br />Functional suites suffer most from a lack of clear directive on what they are written for. Adding view tests (testing windows UI/html output) which cannot be tested by your regular unit tests to your functional suite is a recipe for disaster. View tests do not belong in the function suite. Unfortunately such tests form almost half of the suite. Coupling these not only increases the run time of a suite, it also mandates that the same testing tool is used for both these sets. Think of an ASP.Net website you are developing. A view test suite can be written using the lightening fast NUnitASP toolset, because you wouldnt need to attack cross browser compatibility issues and integration between your user interface and services, and you can write your functional suite with Selenium or your favorite browser based testing tool. Also, view tests should be a part of the tests that a pair runs before they check-in, while all functional tests might not (teams generally decide on a subset as smoke), so dividing your tests judiciously between view tests and functional tests is of utmost importance.<br /><br /><span class="Apple-style-span" style="text-decoration: underline;">Learning</span>: There should be a clear divide between regression functional tests and view tests. Before adding a new test, ask yourself whether you want to test just the view or integration and functionality. <div><br /><span class="Apple-style-span" style="font-weight: bold;">Performance</span><br />Functional tests need to be extremely fast. If not, then scalable with or without parallelization. The functional suite has more often than not the longest feedback cycle on any project. The longer running time forces teams to not include this feedback in their primary builds. A functional suite that takes 12 hours to run is not going to help you a lot with continuous integration. Its not just the wait that hurts, its the lack of granularity at the individual check-in level on how new development affects existing functionality.<br />If the testing tools you are using are not extremely fast, you must look at creating parallel streams of tests to be run. Continuous Integration tools like Cruise with agent based architectures allow you to divide tests in separate meaningful suites and parallelize their run. For browser based applications, tools like Selenium Grid help you do that a level lower than CI (if you use selenium for your tests), and are an excellent choice for distributing your tests across multiple machines. But before you jump onto this, your suite must be ready to be distributed. A classic example of bad design is when you login with one users credentials in all your tests. Most sites/applications dont allow this and hence you cannot parallelize such tests (this one is easy enough to fix, but the team should be on the lookout for such issues). An idea that I am trying to push within ThoughtWorks is to have a small SeleniumRC cloud, which any developer can use to run their tests, hosting the server on their machine (this can be taken further to have a SETI like use hardware when idle approach, but I will be happy with 10 regular boxes dedicated right now). This would make the feedback cycle shorter and probably help include regression tests as a primary test target rather than a nightly build target.<br /><span class="Apple-style-span" style="text-decoration: underline;"><br /></span></div><div><span class="Apple-style-span" style="text-decoration: underline;">Learning</span>: Design with parallelization in mind. Keep the suite run-time within a decided time span (like say 20 minutes) and keep analyzing + refactoring the suite to keep this below the limit.  <br /><br /></div><div><span class="Apple-style-span" style="font-weight: bold;">Ownership</span><br />Having all your functional tests in one suite makes it very difficult to divide tests into regression tests and specifications. When this happens, developers and analysts are all involved in writing functional tests which is a mess because developers want refactorable tests, written in their favorite language and analysts want an easy way to specify tests and run, preferably a tool with recording support, which developers dont like because they are fragile, and hence analysts are forced into complicated object oriented functional suites which takes them away from their primary concern. Fixing tests broken by new functionality is another ownership issue between developers and analysts.<br />Ideally, the analysis team and the quality team should specify new behavior for a release using tests written with their choice of tools, and the development team should work towards making these tests green. This suite may not be monitored for failure, but must be monitored for progress. This suite doesnt need to be highly performant and can be run per iteration. The development team on the other hand should have its own refactorable regression suite, based on the specifications. Failures in this suite must be monitored closely, and this suite should be written with quick feedback in mind. <br />Ideals aside, its much easier to manage the quality process of a team this way because all suites have their dedicated owners with concrete responsibilities on who runs what, and what runs when. It is easier to track progress, and people who write code are responsible for fixing tests they break by adding new functionality, not just leave them by to be attended to by someone else.<br /><span class="Apple-style-span" style="text-decoration: underline;">Learning</span>: Do not hesitate to use different tools for different goals for suites within a team. Have a specification suite and a regression suite to define clear responsibilities.<br /><br /><br />The most wonderful thing about functional automation is its fun and easy to do. To write an effective suite, one must treat it as one treats the product its supposed to test, with clear design goals and responsibilities, an eye on performance every now and then, and effective reporting on progress. Dont leave it as the dark side of the project, but as the release mechanism that gives you confidence.<br /><br /></div></div>

]]></content>
    
  </entry>
  
</feed>